# -*- coding: utf-8 -*-
"""financial_risk_helper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z8eAJwcS21aogAthrpoOOoKBSB0CfD1q

## Model Prediksi Risiko Kredit (Credit Risk Prediction)

## Import Library

Tahapan pertama adalah melakukan import untuk seluruh library yang akan digunakan.
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import shap
from sklearn.svm import SVC
from imblearn.over_sampling import SMOTE
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils.class_weight import compute_class_weight
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

"""## Data Loading

Tahap kedua adalah tahap data loading, di mana data yang diambil dari sumber (https://www.kaggle.com/datasets/preethamgouda/financial-risk) akan dimasukan ke dalam dataframe.
"""

df = pd.read_csv("financial_risk_assessment.csv")

"""## Exploratory Data Analysis

Tahap ketiga adalah tahapan melihat statistik dari dataset yang sudah diambil, apakah ada nilai null dan duplicate atau tidak. Selain itu, kita juga dapat melakukan eksplorasi mengenai dataset yang kita miliki.

menggunakan .head() untuk menampilkan 5 baris pertama dari DataFrame bernama df
"""

df.head()

"""Menggunakan .info() untuk memberikan ringkasan informasi tentang DataFrame df"""

df.info()

"""Menggunakan .describe untuk memberikan ringkasan statistik deskriptif dari kolom-kolom numerik dalam DataFrame df."""

df.describe()

"""Menggunakan .isna() dan .sum() untuk mengecek jumlah nilai missing (atau NaN) di setiap kolom DataFrame df"""

df.isna().sum()

"""Menggunakan .duplicated() dan .sum() untuk mengecek jumlah dumplikasi di setiap kolom DataFrame df"""

df.duplicated().sum()

"""Kemudian membuat **boxplot** menggunakan library seaborn (`sns`) untuk memvisualisasikan distribusi **Credit Score** berdasarkan kategori **Risk Rating** pada DataFrame `df`.

* `x='Risk Rating'` berarti sumbu x menunjukkan kategori Risk Rating (misalnya rendah, sedang, tinggi atau nilai kategorikal lain).
* `y='Credit Score'` berarti sumbu y menampilkan nilai numerik Credit Score.
* Boxplot akan menunjukkan median, kuartil, dan potensi outlier Credit Score pada tiap kategori Risk Rating.
* `plt.title()` memberi judul grafik, dan `plt.show()` menampilkan plotnya.
"""

sns.boxplot(x='Risk Rating', y='Credit Score', data=df)
plt.title('Credit Score vs Risk Rating')
plt.show()

"""Kemudian membuat **countplot** menggunakan seaborn (`sns`) untuk menampilkan frekuensi (jumlah) data pada tiap kategori **Risk Rating**, dan membaginya berdasarkan **Employment Status** sebagai *hue* (warna pembeda).

* `x='Risk Rating'` menampilkan kategori Risk Rating pada sumbu x.
* `hue='Employment Status'` membagi setiap bar Risk Rating menjadi beberapa warna berdasarkan status pekerjaan (misalnya employed, unemployed, dll).
* `plt.title()` memberikan judul grafik, dan `plt.show()` menampilkan visualisasinya.
"""

sns.countplot(x='Risk Rating', hue='Employment Status', data=df)
plt.title('Employment Status by Risk Rating')
plt.show()

"""Kemudian membuat **heatmap** korelasi antar fitur numerik di DataFrame `df` menggunakan seaborn (`sns`), dengan visualisasi sebagai berikut:

* `df.corr(numeric_only=True)` menghitung matriks korelasi Pearson antar kolom numerik saja.
* `sns.heatmap(..., annot=True, cmap='coolwarm')` menggambar peta warna (heatmap) yang menunjukkan nilai korelasi antar fitur, dengan angka korelasi ditampilkan (`annot=True`), dan menggunakan skema warna dari biru ke merah (`coolwarm`) untuk membedakan korelasi negatif dan positif.
* `plt.figure(figsize=(12, 8))` mengatur ukuran gambar supaya cukup besar dan jelas.
* `plt.title()` memberikan judul grafik.
* `plt.show()` menampilkan heatmap.
"""

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix (Numerical Features)')
plt.show()

"""Kemudian membuat **countplot** menggunakan seaborn (`sns`) untuk menampilkan distribusi frekuensi setiap kategori pada kolom **Risk Rating** di DataFrame `df`.

* `x='Risk Rating'` berarti di sumbu x ditampilkan kategori Risk Rating (misalnya rendah, sedang, tinggi).
* Setiap batang menunjukkan berapa banyak data yang termasuk di tiap kategori Risk Rating.
* `plt.title()` memberi judul grafik supaya jelas konteksnya.
* `plt.show()` menampilkan plot tersebut.
"""

sns.countplot(x='Risk Rating', data=df)
plt.title('Distribution of Risk Rating')
plt.show()

"""Dari hasil Asessing data, terdapat total 20 kolom, yaitu:
- Age : Usia individu dalam tahun. Merupakan variabel numerik kontinu yang dapat memengaruhi stabilitas keuangan dan kemampuan membayar pinjaman.
- Gender : Jenis kelamin individu. Kategori mencakup Male (Laki-laki), Female (Perempuan), dan Non-binary (Tidak terikat gender biner). Ini bisa berperan dalam analisis demografis dan pola risiko.
- Education Level : Tingkat pendidikan tertinggi yang telah dicapai, misalnya High School (SMA), Bachelor (S1), Master (S2), hingga PhD (Doktor). Tingkat pendidikan seringkali berhubungan dengan kemampuan finansial dan pekerjaan.
- Marital Status : Status pernikahan saat ini, dikategorikan sebagai Single (Belum menikah), Married (Menikah), Divorced (Bercerai), atau Widowed (Janda/Duda). Status ini dapat memengaruhi tanggungan dan stabilitas finansial.
- Income : Pendapatan tahunan dalam satuan USD. Mewakili kemampuan penghasilan individu untuk membayar kembali pinjaman.
- Credit Score : Skor kredit numerik yang menunjukkan kelayakan kredit individu, biasanya dalam rentang 600 sampai 800. Skor ini sering dipakai lembaga keuangan untuk menilai risiko kredit.
- Loan Amount : Jumlah pinjaman yang diminta oleh individu, dalam satuan uang. Ini merepresentasikan kebutuhan finansial yang ingin dipenuhi melalui pinjaman.
- Loan Purpose : Tujuan penggunaan pinjaman, dikategorikan sebagai Home (Rumah), Auto (Kendaraan), Personal (Pribadi), atau Business (Usaha). Tujuan ini bisa berdampak pada risiko peminjaman.
- Employment Status : Status pekerjaan saat ini, meliputi Employed (Bekerja), Unemployed (Menganggur), atau Self-employed (Wiraswasta). Status ini memberi gambaran tentang stabilitas penghasilan.
- Years at Current Job : Lama waktu individu bekerja di pekerjaan saat ini (dalam tahun). Durasi ini dapat mencerminkan kestabilan kerja dan potensi risiko kredit.

---

Dari hasil Asessing data juga dapat dilihat bahwa ada total 2250 row yang memiliki nilai null serta 0 duplicate. Selain itu tipe data dari masing-masing kolom sudah sesuai.

---

Berdasarkan keempat grafik EDA yang ditampilkan, didapatkan kesimpulan berikut:

## Distribusi Risiko
- **Low Risk**: 9,000 orang (mayoritas)
- **Medium Risk**: 4,500 orang
- **High Risk**: 1,500 orang (minoritas)

## Temuan Utama

**Status Pekerjaan vs Risiko:**
- Semua kategori risiko memiliki distribusi employment status yang relatif seimbang
- Tidak ada perbedaan signifikan antara unemployed, employed, dan self-employed dalam hal tingkat risiko

**Credit Score vs Risiko:**
- Credit score hampir identik di semua kategori risiko (median ~700)
- Rentang dan distribusi credit score sangat mirip, menunjukkan credit score mungkin bukan prediktor kuat untuk risk rating dalam dataset ini

**Korelasi Antar Variabel:**
- Korelasi antar fitur numerik sangat lemah (hampir semua nilai mendekati 0)
- Tidak ada hubungan linear yang kuat antar variabel
- Fitur-fitur tampak independen satu sama lain

## Kesimpulan
Dataset menunjukkan mayoritas aplikan memiliki risiko rendah. Yang menarik, credit score tidak menunjukkan perbedaan signifikan antar kategori risiko, dan employment status juga terdistribusi merata. Korelasi yang sangat lemah antar variabel menunjukkan kompleksitas dalam prediksi risiko kredit mungkin memerlukan analisis non-linear atau faktor lain yang tidak terlihat dalam visualisasi ini.

## Data Cleaning

### Penanganan nilai null

Karena terdapat nilai null di dataset, maka selanjutnya akan dilakukan penanganan terhadap nilai null. Karena data yang memiliki nilai null terlalu banyak (15% dari data asli), maka akan dilakukan imputasi dengan mediannya.

Kemudian melakukan **imputasi nilai hilang (missing values)** pada kolom numerik tertentu di DataFrame `df` menggunakan median sebagai pengganti nilai kosong:

* `num_cols_missing` berisi daftar kolom numerik yang memiliki nilai hilang dan perlu diisi.
* `SimpleImputer(strategy='median')` dari scikit-learn membuat objek imputasi yang mengganti nilai kosong dengan median kolom masing-masing.
* `imputer.fit_transform(df[num_cols_missing])` menghitung median dari tiap kolom dan langsung mengisi nilai hilang di DataFrame tersebut.
* Baris terakhir `print(df[num_cols_missing].isnull().sum())` memastikan bahwa semua nilai hilang sudah terisi (harusnya hasilnya 0 untuk semua kolom).
"""

num_cols_missing = ['Income', 'Credit Score', 'Loan Amount', 'Assets Value', 'Number of Dependents', 'Previous Defaults']

imputer = SimpleImputer(strategy='median')
df[num_cols_missing] = imputer.fit_transform(df[num_cols_missing])

print(df[num_cols_missing].isnull().sum())

df.isna().sum()

"""Hasil dari penanganan missing value menunjukan sudah tidak ada nilai null tersisa di dataframe.

### Encoding Fitur Kategorikal

Untuk memudahkan pemodelan, maka selanjutnya adalah melakukan encoding pada fitur kategorikal. Pada tahap ini fitur kategorikal dan numerik akan dipisah dan dilakukan encoding.

Kemudian melakukan **label encoding** untuk mengubah fitur kategorikal dalam DataFrame `df` menjadi format numerik, yang dibutuhkan oleh sebagian besar algoritma machine learning. Penjelasannya:

* `categorical_cols`: daftar kolom kategorikal yang akan dikodekan.
* `LabelEncoder()` dari `sklearn.preprocessing` digunakan untuk mengubah setiap nilai unik pada kolom menjadi angka integer (misalnya: `Male` jadi `1`, `Female` jadi `0`).
* `le.fit_transform(df[col].astype(str))`: memastikan semua nilai berupa string (menghindari error jika ada `NaN`) lalu mengubahnya jadi angka.
* `label_encoders[col] = le`: menyimpan encoder untuk setiap kolom jika nanti perlu *inverse transform* (mengembalikan ke bentuk aslinya).
* Target kolom `Risk Rating` juga diencode dengan encoder tersendiri (`le_target`).
"""

categorical_cols = [
    'Gender', 'Education Level', 'Marital Status', 'Loan Purpose',
    'Employment Status', 'Payment History', 'City', 'State', 'Country', 'Marital Status Change'
]

label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

le_target = LabelEncoder()
df['Risk Rating'] = le_target.fit_transform(df['Risk Rating'].astype(str))

print(df.head())

"""### Normalisasi kolom numerik

Selain melakukan encoding pada kolom kategorikal, kita juga akan melakukan **normalisasi fitur numerik** menggunakan **MinMaxScaler** dari `sklearn.preprocessing`, yang mengubah nilai dalam setiap kolom numerik ke rentang antara **0 dan 1**. Berikut penjelasannya:

* `numeric_cols`: daftar nama kolom numerik yang akan dinormalisasi.
* `MinMaxScaler()` membuat objek scaler yang akan menghitung nilai minimum dan maksimum tiap kolom.
* `scaler.fit_transform(...)`: menghitung min dan max.
* `df[numeric_cols] = ...`: hasil normalisasi ditimpa kembali ke DataFrame `df` pada kolom-kolom tersebut.
* `print(df.head())`: menampilkan 5 baris pertama untuk memverifikasi hasil scaling.
"""

numeric_cols = [
    'Age', 'Income', 'Credit Score', 'Loan Amount', 'Years at Current Job',
    'Debt-to-Income Ratio', 'Assets Value', 'Number of Dependents', 'Previous Defaults'
]

scaler = MinMaxScaler()

df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

print(df.head())

"""### Data splitting

Di tahap ini kita membagi dataset kita menjadi dua bagian, dataset untuk training 80% dan testing 20%. Ini bertujuan agar model tidak overfitting karena menggunakan dataset yg sama antara training dan testing.
"""

X = df.drop('Risk Rating', axis=1)
y = df['Risk Rating']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

"""## **Model Deployment**

## Training

### Model menggunakan XGBoost

Kode di bawah digunakan untuk melatih model **XGBoost Classifier** dengan penanganan ketidakseimbangan kelas menggunakan bobot sampel. Pertama, bobot kelas dihitung secara otomatis menggunakan `compute_class_weight` berdasarkan distribusi label `y_train_resampled` hasil SMOTE. Bobot tersebut kemudian dipetakan ke setiap sampel menggunakan `map()` dan disimpan dalam `sample_weight`. Model XGBoost kemudian diinisialisasi dengan metrik evaluasi `'mlogloss'` dan dilatih menggunakan data `X_train_resampled` dan `y_train_resampled` bersama `sample_weight`. Pendekatan ini membantu model untuk lebih memperhatikan kelas minoritas dan meningkatkan performa prediksi dalam kasus klasifikasi dengan data tidak seimbang.
"""

class_weights = compute_class_weight(class_weight='balanced',
                                     classes=np.unique(y_train_resampled),
                                     y=y_train_resampled)
weights_dict = dict(zip(np.unique(y_train_resampled), class_weights))

sample_weights = pd.Series(y_train_resampled).map(weights_dict)

model_xgb = XGBClassifier(eval_metric='mlogloss', random_state=42)

model_xgb.fit(X_train_resampled, y_train_resampled, sample_weight=sample_weights)

"""### Model menggunakan Random Forest

Kode di bawah membuat dan melatih model *Random Forest Classifier* dengan 200 pohon keputusan (*n\_estimators=200*) dan kedalaman maksimum tiap pohon sebesar 10 (*max\_depth=10*). Parameter *min\_samples\_leaf=5* memastikan bahwa setiap daun pohon memiliki minimal 5 sampel, yang membantu mengurangi overfitting. *class\_weight='balanced'* digunakan untuk menangani ketidakseimbangan kelas dengan menyesuaikan bobot kelas secara otomatis berdasarkan frekuensinya. Model ini dilatih menggunakan data *X\_train\_resampled* dan labelnya *y\_train\_resampled*, yang kemungkinan telah di-*resample* untuk mengatasi ketidakseimbangan data. *random\_state=42* digunakan agar hasilnya dapat direproduksi.
"""

rf = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    min_samples_leaf=5,
    class_weight='balanced',
    random_state=42
)

rf.fit(X_train_resampled, y_train_resampled)

"""### Model menggunakan SVM

Kode di bawah membuat dan melatih model *Support Vector Machine (SVM)* dengan kernel *radial basis function (RBF)*. Parameter *C=1* mengontrol trade-off antara akurasi klasifikasi dan margin pemisah yang lebar—nilai ini memberikan keseimbangan moderat antara keduanya. *decision\_function\_shape='ovr'* menunjukkan bahwa strategi *one-vs-rest* digunakan untuk menangani klasifikasi multikelas. Model kemudian dilatih menggunakan data yang telah di-*resample*, yaitu *X\_train\_resampled* dan *y\_train\_resampled*, untuk mengatasi ketidakseimbangan kelas.
"""

svm_model = SVC(kernel='rbf', C=1, decision_function_shape='ovr', probability=True)
svm_model.fit(X_train_resampled, y_train_resampled)

"""### Model menggunakan Naive Bayes

Kode di bawah membuat dan melatih model *Gaussian Naive Bayes* (NB). Proses pelatihan dilakukan menggunakan data *X\_train\_resampled* dan *y\_train\_resampled*, yang kemungkinan sudah diseimbangkan sebelumnya untuk menangani ketimpangan distribusi kelas.
"""

nb_model = GaussianNB()
nb_model.fit(X_train_resampled, y_train_resampled)

"""## Matrix Evaluasi Model

Selanjutnya adalah mengevaluasi dan membandingkan performa empat model klasifikasi berbeda — **XGBoost (model\_xgb)**, **Random Forest (rf)**, **SVM (svm\_model)**, dan **Gaussian Naive Bayes (nb\_model)** — terhadap data uji (*X\_test* dan *y\_test*).

Untuk setiap model, dilakukan langkah-langkah berikut:

1. **Prediksi hasil kelas** pada data uji menggunakan `predict()`.
2. **Akurasi** model dicetak dengan `accuracy_score()`, yang menunjukkan seberapa sering model memprediksi label dengan benar.
3. **Classification report** dicetak menggunakan `classification_report()`, yang memuat metrik seperti precision, recall, f1-score, dan support untuk masing-masing kelas. Argumen `zero_division=0` digunakan untuk menghindari error jika ada pembagian dengan nol.
4. **Confusion matrix** ditampilkan dengan `confusion_matrix()`, yang menunjukkan jumlah prediksi benar dan salah untuk setiap kelas, membantu dalam menganalisis kesalahan klasifikasi.

Tujuan utama kode ini adalah untuk membandingkan performa keempat model menggunakan metrik evaluasi yang komprehensif.
"""

y_pred = model_xgb.predict(X_test)
print("XGB Accuracy:", accuracy_score(y_test, y_pred))
print("XGB Classification Report:\n", classification_report(y_test, y_pred, zero_division=0))
print("XGB Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

y_pred = rf.predict(X_test)
print("RF Accuracy:", accuracy_score(y_test, y_pred))
print("RF Classification Report:\n", classification_report(y_test, y_pred, zero_division=0))
print("RF Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

y_pred_svm = svm_model.predict(X_test)
print("SVM Classification Report:\n", classification_report(y_test, y_pred_svm))
print("SVM Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))

y_pred_nb = nb_model.predict(X_test)
print("Naive Bayes Classification Report:\n", classification_report(y_test, y_pred_nb))
print("Naive Bayes Confusion Matrix:\n", confusion_matrix(y_test, y_pred_nb))

"""Dari hasil evaluasi keempat model pada data uji, terlihat perbedaan performa yang cukup signifikan:

* **XGBoost (XGB)** memiliki akurasi tertinggi sekitar 55%, dengan performa terbaik pada kelas 1 (precision 0.60, recall 0.84, f1-score 0.70). Namun, kelas 0 dan 2 memiliki performa rendah, terutama kelas 0 dengan recall hanya 0.02, yang berarti hampir sebagian besar sampel kelas 0 salah diklasifikasikan. Confusion matrix menunjukkan model ini sering salah mengklasifikasikan kelas 0 dan 2 sebagai kelas 1.

* **Random Forest (RF)** akurasinya lebih rendah, sekitar 52%. Sama seperti XGB, model ini juga lebih baik dalam mengenali kelas 1 (recall 0.81), tapi performa kelas 0 dan 2 masih lemah, terutama kelas 2 dengan recall hanya 0.06. Confusion matrix memperlihatkan banyak kesalahan klasifikasi kelas 0 dan 2 ke kelas 1.

* **SVM** memiliki akurasi paling rendah, hanya sekitar 26%. Model ini cenderung salah memprediksi kelas 1 dan 2 dengan sangat tinggi (banyak sampel kelas 1 salah diklasifikasikan ke kelas 0 dan 2, serta kelas 2 sering salah prediksi). Recall untuk kelas 1 sangat rendah (0.14), yang artinya SVM kesulitan mengenali kelas mayoritas ini. Namun, recall kelas 0 dan 2 relatif lebih baik dibanding kelas 1, menunjukkan bias yang berbeda dari model pohon.

* **Naive Bayes (NB)** punya akurasi 42%, performanya seimbang tapi relatif rendah di semua kelas. Recall kelas 1 masih moderat (0.57), tapi kelas 0 dan 2 masih rendah (0.19 dan 0.21). Model ini juga sering salah memprediksi kelas 0 dan 2 ke kelas 1, mirip dengan pola pada model pohon.

**Kesimpulan:**
Model XGBoost menunjukkan performa terbaik dalam mengenali kelas mayoritas (kelas 1), tapi semua model masih mengalami kesulitan signifikan dalam mengklasifikasikan kelas minoritas (kelas 0 dan 2), yang mungkin disebabkan oleh ketidakseimbangan data atau fitur yang kurang informatif. SVM tampaknya tidak cocok dengan distribusi data ini karena akurasi dan recall kelas mayoritas sangat rendah. Perlu usaha lebih lanjut seperti tuning hyperparameter, teknik penyeimbangan data yang lebih efektif, atau fitur engineering untuk meningkatkan performa model terutama pada kelas minoritas.
"""

# Contoh batch data baru
new_data = pd.DataFrame([
    {
        'Gender': 'Female',
        'Education Level': 'Master',
        'Marital Status': 'Married',
        'Loan Purpose': 'Business',
        'Employment Status': 'Self-employed',
        'Payment History': 'Average',
        'City': 'Bandung',
        'State': 'Jawa Barat',
        'Country': 'Indonesia',
        'Marital Status Change': 'Yes',
        'Age': 35,
        'Income': 8000,
        'Credit Score': 690,
        'Loan Amount': 15000,
        'Years at Current Job': 5,
        'Debt-to-Income Ratio': 0.25,
        'Assets Value': 30000,
        'Number of Dependents': 1,
        'Previous Defaults': 0
    },
    {
        'Gender': 'Male',
        'Education Level': 'Bachelor',
        'Marital Status': 'Single',
        'Loan Purpose': 'Education',
        'Employment Status': 'Employed',
        'Payment History': 'Good',
        'City': 'Jakarta',
        'State': 'DKI Jakarta',
        'Country': 'Indonesia',
        'Marital Status Change': 'No',
        'Age': 28,
        'Income': 5000,
        'Credit Score': 720,
        'Loan Amount': 10000,
        'Years at Current Job': 2,
        'Debt-to-Income Ratio': 0.18,
        'Assets Value': 20000,
        'Number of Dependents': 0,
        'Previous Defaults': 0
    },
    {
        'Gender': 'Female',
        'Education Level': 'PhD',
        'Marital Status': 'Divorced',
        'Loan Purpose': 'Home',
        'Employment Status': 'Unemployed',
        'Payment History': 'Poor',
        'City': 'Surabaya',
        'State': 'Jawa Timur',
        'Country': 'Indonesia',
        'Marital Status Change': 'Yes',
        'Age': 45,
        'Income': 3000,
        'Credit Score': 550,
        'Loan Amount': 20000,
        'Years at Current Job': 0,
        'Debt-to-Income Ratio': 0.4,
        'Assets Value': 10000,
        'Number of Dependents': 3,
        'Previous Defaults': 2
    }
])

# ======== Preprocessing ========

# Encode fitur kategorikal
for col in categorical_cols:
    le = label_encoders[col]
    new_data[col] = new_data[col].astype(str).map(
        lambda x: le.transform([x])[0] if x in le.classes_ else -1
    )

# Scaling fitur numerik
new_data[numeric_cols] = scaler.transform(new_data[numeric_cols])

# Urutkan sesuai kolom training
X_input = new_data[X.columns]

# ======== Prediksi dari Semua Model ========
xgb_preds = model_xgb.predict(X_input)
xgb_labels = le_target.inverse_transform(xgb_preds)

rf_preds = rf.predict(X_input)
rf_labels = le_target.inverse_transform(rf_preds)

svm_preds = svm_model.predict(X_input)
svm_labels = le_target.inverse_transform(svm_preds)

nb_preds = nb_model.predict(X_input)
nb_labels = le_target.inverse_transform(nb_preds)

# ======== Gabungkan dan Tampilkan Hasil ========
results = new_data.copy()
results['XGB Prediction'] = xgb_labels
results['RF Prediction'] = rf_labels
results['SVM Prediction'] = svm_labels
results['NB Prediction'] = nb_labels

print(results[[
    'XGB Prediction', 'RF Prediction', 'SVM Prediction', 'NB Prediction'
]])